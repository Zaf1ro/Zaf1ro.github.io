<!DOCTYPE html>
<html lang="zh">
  <head>
    
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1">



  <meta name="description" content="Replication Controllers"/>




  <meta name="keywords" content="K8s," />


<meta name="google-site-verification" content="qy4gf0StWj627u-7aIP3WDCLBi3jPYXhm57UC7TUcok" />
<meta name="baidu-site-verification" content="XJoPG7ad2Z" />

<link rel="alternate" hreflang="x-default" href="https://zaf1ro.github.io/p/17e0.html" />
<link rel="alternate" hreflang="zh" href="https://zaf1ro.github.io/p/17e0.html" />




  <link rel="alternate" href="/default" title="Zaf1ro" >




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.jpeg?v=1.1" />



<link rel="canonical" href="https://zaf1ro.github.io/p/17e0.html"/>


<meta name="description" content="1. Introduction当k8s API server接收到pod请求时, k8s会将pod分配到一个worker node并开始运行. 虽然kubelet可通过各种probe(liveness probe, readiness probe, startup probe)检测pod中container的状态, 但pod只会被部署一次, 因此一旦pod所在的node不可用, 则kubelet和">
<meta property="og:type" content="article">
<meta property="og:title" content="Replication Controllers">
<meta property="og:url" content="https://zaf1ro.github.io/p/17e0.html">
<meta property="og:site_name" content="Zaf1ro">
<meta property="og:description" content="1. Introduction当k8s API server接收到pod请求时, k8s会将pod分配到一个worker node并开始运行. 虽然kubelet可通过各种probe(liveness probe, readiness probe, startup probe)检测pod中container的状态, 但pod只会被部署一次, 因此一旦pod所在的node不可用, 则kubelet和">
<meta property="og:locale">
<meta property="og:image" content="https://zaf1ro.github.io/images/Kubernetes/replication-3-1.png">
<meta property="og:image" content="https://zaf1ro.github.io/images/Kubernetes/replication-3-2.png">
<meta property="og:image" content="https://zaf1ro.github.io/images/Kubernetes/replication-3-3.png">
<meta property="og:image" content="https://zaf1ro.github.io/images/Kubernetes/replication-3-4.png">
<meta property="og:image" content="https://zaf1ro.github.io/images/Kubernetes/replication-3-5.png">
<meta property="og:image" content="https://zaf1ro.github.io/images/Kubernetes/replication-3-6.png">
<meta property="og:image" content="https://zaf1ro.github.io/images/Kubernetes/replication-5-1.png">
<meta property="og:image" content="https://zaf1ro.github.io/images/Kubernetes/replication-5-2.png">
<meta property="og:image" content="https://zaf1ro.github.io/images/Kubernetes/replication-6-1.png">
<meta property="article:published_time" content="2019-10-17T12:32:29.000Z">
<meta property="article:modified_time" content="2025-01-10T00:37:23.217Z">
<meta property="article:tag" content="K8s">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zaf1ro.github.io/images/Kubernetes/replication-3-1.png">


<link rel="stylesheet" type="text/css" href="/css/style.css?v=1.1" />




    <title>
Replication Controllers - Zaf1ro
</title>
  <meta name="generator" content="Hexo 6.3.0"></head>

  <body>
  <nav id="sidebar" class="active on-post">
    <div id="third">
      <div id="sidebar-title">
        <h1 id="sidebar-title-text">
            <a href="/." class="logo">Home</a>
        </h1>
      </div>
      <div id="google-search">
  <script async src="https://cse.google.com/cse.js?cx=009060789867951546370:v3hkcobeuh9"></script>
  <div class="gcse-search"></div>
</div>
      
  <div id="toc">
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Introduction"><span class="toc-text">1. Introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-Probes-for-container"><span class="toc-text">1.1 Probes for container</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Liveness-probes"><span class="toc-text">2. Liveness probes</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Create-an-HTTP-based-liveness-probe"><span class="toc-text">2.1 Create an HTTP-based liveness probe</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Configure-additional-properties-of-the-liveness-probe"><span class="toc-text">2.2 Configure additional properties of the liveness probe</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Create-effective-liveness-probes"><span class="toc-text">2.3 Create effective liveness probes</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-ReplicationControllers"><span class="toc-text">3. ReplicationControllers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-The-Operation-of-a-ReplicationController"><span class="toc-text">3.1 The Operation of a ReplicationController</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Create-a-ReplicationController"><span class="toc-text">3.2 Create a ReplicationController</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-ReplicationController-in-action"><span class="toc-text">3.3 ReplicationController in action</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-Move-pods-in-and-out-of-the-scope-of-a-ReplicationController"><span class="toc-text">3.4 Move pods in and out of the scope of a ReplicationController</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-Change-the-pod-template"><span class="toc-text">3.5 Change the pod template</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-Horizontally-scaling-pods"><span class="toc-text">3.6 Horizontally scaling pods</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-7-Delete-a-ReplicationController"><span class="toc-text">3.7 Delete a ReplicationController</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-ReplicaSet"><span class="toc-text">4. ReplicaSet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-DaemonSet"><span class="toc-text">5. DaemonSet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Job"><span class="toc-text">6. Job</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-Define-a-Job-resource"><span class="toc-text">6.1 Define a Job resource</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-Seeing-a-Job-run-a-Pod"><span class="toc-text">6.2 Seeing a Job run a Pod</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-Run-multiple-pod-instance-in-a-Job"><span class="toc-text">6.3 Run multiple pod instance in a Job</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-CronJob"><span class="toc-text">7. CronJob</span></a></li></ol>
  </div>

    </div>
  </nav>

    <div id="page">
      <header id="masthead"><div class="site-header-inner">
  
  <button class="nav-mobile-button on-post" id="sidebarCollapse">
  
    <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24"><path d="M24 6h-24v-4h24v4zm0 4h-24v4h24v-4zm0 8h-24v4h24v-4z"/></svg>
  </button>
  
  


  <nav id="nav-top">
    
      
      <ul id="menu-top" class="nav-top-items on-post">
      
        
          <li class="menu-item">
            <a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/Zaf1ro">
              
              
                Github
              
            </a>
          </li>
        
      </ul>
    
  </nav>
</div>

      </header>
      <div id="content">
        
  <div class="primary">
    
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          Replication Controllers
        
      </h1>
      <time class="post-time">
          10/17/19
      </time>
    </header>

    <div class="post-content">
      <h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>当k8s API server接收到pod请求时, k8s会将pod分配到一个worker node并开始运行. 虽然kubelet可通过各种probe(liveness probe, readiness probe, startup probe)检测pod中container的状态, 但pod只会被部署一次, 因此一旦pod所在的node不可用, 则kubelet和pod都不可用, k8s也不会重新部署该pod. 在解决node不可用之前, 我们需先了解kubelet如何处理pod中container的不可用情况:</p>
<ul>
<li>若container只有一个进程, 当该进程停止运行时, kubelet会自动重启container</li>
<li>若container存在多个进程, 当<strong>主进程</strong>(dockerfile中<code>ENTRYPOINT</code>或<code>CMD</code>启动的进程)停止运行时, kebelet会自动重启container</li>
</ul>
<p>但很多情况下, container中的进程处于<strong>不可用</strong>但<strong>未停止运行</strong>的状态, 例如: 当Java程序内存不足时会抛出<code>OutOfMemoryErrors</code>, 但JVM进程依然运行, 导致kubelet不会重启container. 因此需要一种机制来告诉kubelet何时重启container.</p>
<h3 id="1-1-Probes-for-container"><a href="#1-1-Probes-for-container" class="headerlink" title="1.1 Probes for container"></a>1.1 Probes for container</h3><p>K8s提供了三种probe(探针)来检测container状态:</p>
<ul>
<li>Liveness probe: 决定是否重启container. 大部分情况下会将liveness probe设置为一个HTTP endpoint, 当container中进程陷入死锁时, k8s在尝试访问多次(failureThreshold)endpoint后会判定container不可用, 并重启container.</li>
<li>Readiness probe: 决定container是否接受流量, 通常用于等待一些耗时的初始化任务, 例如: 建立网络连接, 加载文件, 初始化缓存.</li>
<li>Startup probe: 决定container是否启动. 通常和liveness probe一起使用, 避免container被提前重启.</li>
</ul>
<h2 id="2-Liveness-probes"><a href="#2-Liveness-probes" class="headerlink" title="2. Liveness probes"></a>2. Liveness probes</h2><p>K8s可通过liveness probe检测container是否可用. Liveness probe有三种探测方式:</p>
<ul>
<li>HTTP GET probe向container的IP address发送GET请求. 若probe收到的response code为2xx或3xx, 则认为container可用; 否则重启container.</li>
<li>TCP Socket probe会与container的指定port发起TCP连接. 若成功创建连接, 则container可用; 否则重启container.</li>
<li>Exec probe在container中执行指定命令, 并检查exit status code. 若为0, 则container可用; 否则重启container.</li>
</ul>
<h3 id="2-1-Create-an-HTTP-based-liveness-probe"><a href="#2-1-Create-an-HTTP-based-liveness-probe" class="headerlink" title="2.1 Create an HTTP-based liveness probe"></a>2.1 Create an HTTP-based liveness probe</h3><p>创建pod时, 可为每个container指定一个liveness probe. 下例中, probe每隔一段时间(默认10秒)会对名为<code>kubia-liveness</code>的pod所在的IP address下的<code>/</code>路径和<code>8080</code>端口进行HTTP访问.</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kubia-liveness</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">luksa/kubia-unhealthy</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">kubia</span></span><br><span class="line">    <span class="attr">livenessProbe:</span></span><br><span class="line">      <span class="attr">httpGet:</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">/</span></span><br><span class="line">        <span class="attr">port:</span> <span class="number">8080</span> </span><br></pre></td></tr></table></figure>
<p>假设container返回500, k8s会自动重启该container. 执行<code>kubectl get pods</code>时, <strong>RESTARTS</strong>一列会显示该pod中的container重启了多少次:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl get po kubia-liveness</span><br><span class="line">NAME     READY STATUS  RESTARTS AGE</span><br><span class="line">pod-name 1/1   Running 1        2m</span><br></pre></td></tr></table></figure>
<p>执行<code>kubectl describe pod</code>可获得probe更详细的信息:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl describe po kubia-liveness</span><br><span class="line">Name: kubia-liveness</span><br><span class="line">...</span><br><span class="line">Containers:</span><br><span class="line">  kubia:</span><br><span class="line">  Container ID:   docker://480986f8</span><br><span class="line">  Image:          luksa/kubia-unhealthy</span><br><span class="line">  Image ID:       docker://sha256:2b208508</span><br><span class="line">  Port:</span><br><span class="line">  State:          Running</span><br><span class="line">    Started:      Sun, 14 May 2017 11:41:40 +0200</span><br><span class="line">  Last State:     Terminated</span><br><span class="line">    Reason:       Error</span><br><span class="line">    Exit Code:    137</span><br><span class="line">    Started:      Mon, 01 Jan 0001 00:00:00 +0000</span><br><span class="line">    Finished:     Sun, 14 May 2017 11:41:38 +0200</span><br><span class="line">  Ready:          True</span><br><span class="line">  Restart Count:  1</span><br><span class="line">  Liveness:       http-get http://:8080/ delay=0s <span class="built_in">timeout</span>=1s</span><br><span class="line">                  period=10s <span class="comment">#success=1 #failure=3</span></span><br><span class="line">  ...</span><br><span class="line">Events:</span><br><span class="line">  ... Killing container with <span class="built_in">id</span> docker://95246981:pod <span class="string">&quot;kubia-liveness ...&quot;</span></span><br><span class="line">  container <span class="string">&quot;kubia&quot;</span> is unhealthy, it will be killed and re-created.</span><br></pre></td></tr></table></figure>
<p>上述表示当前运行的container之前被重启过, exit code为137. 退出码为128+x, x表示发送给进程的信号, 也即是<code>SIGKILL</code>. </p>
<h3 id="2-2-Configure-additional-properties-of-the-liveness-probe"><a href="#2-2-Configure-additional-properties-of-the-liveness-probe" class="headerlink" title="2.2 Configure additional properties of the liveness probe"></a>2.2 Configure additional properties of the liveness probe</h3><p>Liveness probe还可以配置其他属性:</p>
<ul>
<li>initialDelaySeconds: container启动后多久开始probe, 默认为0</li>
<li>timeoutSeconds: probe发出请求后可等待多久回复, 默认为1</li>
<li>periodSeconds: probe的请求间隔, 默认为10</li>
<li>failureThreshold: probe探测到container失败后, 重启container的最大次数, 默认为3, 最小为1.</li>
</ul>
<p>HTTP GET probe提供了以下属性:</p>
<ul>
<li>host: Hostname, 默认为pod的IP address</li>
<li>scheme: 连接container的方式, HTTP或HTTPS, 默认为HTTP</li>
<li>path: HTTP资源路径</li>
<li>httpHeaders: HTTP Header</li>
<li>port: HTTP port, 范围为[1, 65535]</li>
</ul>
<h3 id="2-3-Create-effective-liveness-probes"><a href="#2-3-Create-effective-liveness-probes" class="headerlink" title="2.3 Create effective liveness probes"></a>2.3 Create effective liveness probes</h3><p>生产环境中, pod中的每个container都应有一个liveness probe. 但需要注意以下几点:</p>
<ul>
<li>上述liveness probe例子对于很多项目来说过于简单, 无法覆盖所有应用面对的问题, 通常会创建一个单独应用用于检测进程是否正常运行, probe只需访问一个专属URL地址(如<code>/health</code>).</li>
<li>Liveness probe只应检测container内部产生的异常, 而不是外部因素导致的异常, 例如: 当使用liveness probe检测web server运行状况时, 若因数据库问题或其他外部依赖而停止运行, 则不应该让server不断重启. </li>
<li>Liveness probe的设计必须轻量级, 不应占据太多计算资源, 因为每个container都有资源上限, 且liveness probe与container共享计算资源.</li>
</ul>
<p>Probe由worker node上的kubelet管理, k8s control plane并不参与, 因此当worker node不可用时, probe也不再可用. 为保证node不可用时, k8s会将该node上所有pod转移到其他worker node上, 需使用ReplicationController或其他机制.</p>
<h2 id="3-ReplicationControllers"><a href="#3-ReplicationControllers" class="headerlink" title="3. ReplicationControllers"></a>3. ReplicationControllers</h2><p>ReplicationController作为k8s其中一种资源, 用于确保pods一直保持运行. 当pod消失时(pod被删除或pod所在的node不可用), ReplicationController都会创造新的pod. 下图中, Pod B由ReplicationController管理, 而Pod A被直接创建. 当Node 1不可用时, ReplicationController会在Node 2创建一个新的Pod B来替代失踪的Pod B. 以下ReplicationController简称为<strong>RC</strong>.<br><img src="/images/Kubernetes/replication-3-1.png" alt="When a node fails, only pods backed by a ReplicationController are recreated"></p>
<h3 id="3-1-The-Operation-of-a-ReplicationController"><a href="#3-1-The-Operation-of-a-ReplicationController" class="headerlink" title="3.1 The Operation of a ReplicationController"></a>3.1 The Operation of a ReplicationController</h3><p>RC会持续监控pod集合并确保该类型的pod维持在一定数量:</p>
<ul>
<li>若运行中的pod数量小于目标数量, 则创建新的pod</li>
<li>若运行中的pod数量大于目标数量, 则移除多余pod</li>
</ul>
<p>RC的运行过程如下:<br><img src="/images/Kubernetes/replication-3-2.png" alt="A ReplicationController’s reconciliation loop"></p>
<p>可以看出, RC最重要的目的是保持运行中的pod数量始终与目标数量一致, 比较数量的方法就是<strong>label selector</strong>: RC创建的pod一定与其label selector匹配, 因此可称为<strong>同类型pod</strong>. 以下操作会导致运行中的pod数量大于目标数量:</p>
<ul>
<li>创建一个同类型的pod</li>
<li>其他pod被修改为同类型</li>
<li>RC的pod目标数量被下调</li>
</ul>
<p>RC由以下三个部分组成:</p>
<ul>
<li>label selector: RC管理的pod范围</li>
<li>replica count: 所需的运行中pod运行数量</li>
<li>pod template: 创建新pod所需的模板</li>
</ul>
<p><img src="/images/Kubernetes/replication-3-3.png" alt="The three key parts of a ReplicationController (pod selector, replica count, and pod template)"></p>
<p>相比于pod, RC拥有以下优点:</p>
<ul>
<li>Pod消失时, 确保新的Pod会被创建并持续运行</li>
<li>Node不可用时, 会在其他可用node中创建pod</li>
<li>提供了一种水平拓展pod的方式</li>
</ul>
<h3 id="3-2-Create-a-ReplicationController"><a href="#3-2-Create-a-ReplicationController" class="headerlink" title="3.2 Create a ReplicationController"></a>3.2 Create a ReplicationController</h3><p>RC也通过JSON或YAML文件创建:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ReplicationController</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kubia</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">kubia</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">kubia</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">kubia</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">image-name</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">8080</span> </span><br></pre></td></tr></table></figure>
<p>将YAML文件上传到k8s API server后, k8s会创建名为kubia的RC, 并创建三个label为<strong>app&#x3D;kubia</strong>的pod, RC会保证随时都有3个该类型的pod在运行. API server会检查spec.selector与spec.template.metadata.labels是否一致, 为避免label不一致, 可不设置spec.selector, k8s会自动将label selector设置为spec.template.metadata.labels.</p>
<p>K8s允许用户修改label selector, replica count和pod template:</p>
<ul>
<li>修改label selector后, RC会创建新的pod, 之前创建的pod将会被忽略. 需要注意的是, 修改时必须保证<code>spec.selector</code>与<code>spec.template.metadata.labels</code>一致</li>
<li>修改pod template后, 运行中的pod不会有任何改变, 只有RC需要创建新的pod时才会使用新的pod template</li>
<li>修改replica count后, RC会立即比较运行中的pod数量是否符合要求, 若小于目标数量, 则创建新pod; 若大于目标数量, 则删除已有pod</li>
</ul>
<h3 id="3-3-ReplicationController-in-action"><a href="#3-3-ReplicationController-in-action" class="headerlink" title="3.3 ReplicationController in action"></a>3.3 ReplicationController in action</h3><p>执行kubectl create创建RC:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl create -f kubia-rc.yaml</span><br><span class="line">replicationcontroller <span class="string">&quot;kubia&quot;</span> created</span><br><span class="line"></span><br><span class="line">$ kubectl get pods</span><br><span class="line">NAME        READY STATUS            RESTARTS AGE</span><br><span class="line">kubia-53thy 0/1   ContainerCreating 0        2s</span><br><span class="line">kubia-k0xz6 0/1   ContainerCreating 0        2s</span><br><span class="line">kubia-q3vkg 0/1   ContainerCreating 0        2s</span><br></pre></td></tr></table></figure>
<p>当pod数量小于目标pod数量时, RC会自动创建新的pod:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl delete pod kubia-53thy</span><br><span class="line">pod <span class="string">&quot;kubia-53thy&quot;</span> deleted</span><br><span class="line"></span><br><span class="line">$ kubectl get pods</span><br><span class="line">NAME        READY STATUS            RESTARTS AGE</span><br><span class="line">kubia-53thy 1/1   Terminating       0        3m</span><br><span class="line">kubia-oini2 0/1   ContainerCreating 0        2s</span><br><span class="line">kubia-k0xz6 1/1   Running           0        3m</span><br><span class="line">kubia-q3vkg 1/1   Running           0        3m</span><br></pre></td></tr></table></figure>
<p>查询所有的rc:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl get rc</span><br><span class="line">NAME  DESIRED CURRENT READY AGE</span><br><span class="line">kubia 3       3       2     3m</span><br></pre></td></tr></table></figure>
<p>以下是node不可用时的情况(本例中共有3个worker nodes, 3个pods):</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">----- Disconnect one of worker nodes from the network -----</span><br><span class="line"></span><br><span class="line">$ kubectl get node</span><br><span class="line">NAME            STATUS   AGE</span><br><span class="line">kubia-pool-opc5 Ready    5h</span><br><span class="line">kubia-pool-s8gj Ready    5h</span><br><span class="line">kubia-pool-zwko NotReady 5h</span><br><span class="line"></span><br><span class="line">$ kubectl get pods</span><br><span class="line">NAME        READY STATUS  RESTARTS AGE</span><br><span class="line">kubia-oini2 1/1   Running 0        10m</span><br><span class="line">kubia-k0xz6 1/1   Running 0        10m</span><br><span class="line">kubia-q3vkg 1/1   Unknown 0        10m</span><br><span class="line">kubia-dmdck 1/1   Running 0        5s</span><br></pre></td></tr></table></figure>
<p>当worker node重新连接后, pod的status变为Ready, 最终由于pod数量超过目标值, 因此会被删除.</p>
<h3 id="3-4-Move-pods-in-and-out-of-the-scope-of-a-ReplicationController"><a href="#3-4-Move-pods-in-and-out-of-the-scope-of-a-ReplicationController" class="headerlink" title="3.4 Move pods in and out of the scope of a ReplicationController"></a>3.4 Move pods in and out of the scope of a ReplicationController</h3><ol>
<li>Add Labels to Pods Managed by a ReplicationController<br>  RC并不会在意pod上的其他label:  <figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl label pod kubia-dmdck <span class="built_in">type</span>=special</span><br><span class="line">pod <span class="string">&quot;kubia-dmdck&quot;</span> labeled</span><br><span class="line"></span><br><span class="line">$ kubectl get pods --show-labels</span><br><span class="line">NAME        READY STATUS  RESTARTS AGE LABELS</span><br><span class="line">kubia-oini2 1/1   Running 0        11m app=kubia</span><br><span class="line">kubia-k0xz6 1/1   Running 0        11m app=kubia</span><br><span class="line">kubia-dmdck 1/1   Running 0        1m  app=kubia,<span class="built_in">type</span>=special</span><br></pre></td></tr></table></figure></li>
<li>Change the Labels of a Managed Pod<br>  修改pod的label可能导致pod脱离RC的管理范围, 从而导致RC创建一个新的pod:  <figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl label pod kubia-dmdck app=foo --overwrite</span><br><span class="line">pod <span class="string">&quot;kubia-dmdck&quot;</span> labeled</span><br><span class="line"></span><br><span class="line">$ kubectl get pods -L app</span><br><span class="line">NAME        READY STATUS            RESTARTS AGE APP</span><br><span class="line">kubia-2qneh 0/1   ContainerCreating 0        2s  kubia</span><br><span class="line">kubia-oini2 1/1   Running           0        20m kubia</span><br><span class="line">kubia-k0xz6 1/1   Running           0        20m kubia</span><br><span class="line">kubia-dmdck 1/1   Running           0        10m foo </span><br></pre></td></tr></table></figure>
  <img src="/images/Kubernetes/replication-3-4.png" alt="Removing a pod from the scope of a ReplicationController by changing its labels"></li>
<li>Remove Pods from RC in Practice<br>  当pod运行错误时, 可删除该pod, RC会自动创建一个新的pod</li>
<li>Change the ReplicationController&#39;s label selector<br>  修改RC的label selector后, 之前运行的Pod会脱离管理并继续运行, RC会创建新的pod来保持replica count, 但绝大多数情况下都不需要修改label selector</li>
</ol>
<h3 id="3-5-Change-the-pod-template"><a href="#3-5-Change-the-pod-template" class="headerlink" title="3.5 Change the pod template"></a>3.5 Change the pod template</h3><p>修改RC的pod template不会影响已经运行的Pod, 若想让所有运行的pod遵循新的pod template, 需要手动删除所有pods.<br><img src="/images/Kubernetes/replication-3-5.png" alt="Changing a ReplicationController’s pod template only affects pods created afterward and has no effect on existing pods"></p>
<p>执行<code>kubectl edit rc &lt;rc-name&gt;</code>会打开RC的YAML文件, 修改后保存即可. 若需修改editor, 可修改环境变量<code>KUBE_EDUTOR</code>为目标editor路径.</p>
<h3 id="3-6-Horizontally-scaling-pods"><a href="#3-6-Horizontally-scaling-pods" class="headerlink" title="3.6 Horizontally scaling pods"></a>3.6 Horizontally scaling pods</h3><p>有两种方法改变replica count:</p>
<ol>
<li>执行<code>kubectl scale</code>修改replica count:<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl scale rc kubia --replicas=&lt;num-of-desired-replicas&gt;</span><br></pre></td></tr></table></figure></li>
<li>执行<code>kubectl edit</code>修改RC的YAML文件:<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">kubectl edit rc kubia</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="3-7-Delete-a-ReplicationController"><a href="#3-7-Delete-a-ReplicationController" class="headerlink" title="3.7 Delete a ReplicationController"></a>3.7 Delete a ReplicationController</h3><p>执行<code>kubectl delete</code>不仅会删除RC, 还会删除其管理的所有pods. 若想在删除RC的前提下保留其pods, 可加上<code>--cascade=false</code>:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl delete rc kubia --cascade=<span class="literal">false</span></span><br><span class="line">replicationcontroller <span class="string">&quot;kubia&quot;</span> deleted</span><br></pre></td></tr></table></figure>
<p><img src="/images/Kubernetes/replication-3-6.png" alt="Deleting a replication controller with --cascade=false leaves pods unmanaged"></p>
<h2 id="4-ReplicaSet"><a href="#4-ReplicaSet" class="headerlink" title="4. ReplicaSet"></a>4. ReplicaSet</h2><p>最一开始, RC是k8s中唯一可以水平拓展pod的工具, 但之后引入了ReplicaSet, 其可以完全替代RC. ReplicaSet相比于RC的最大区别在于selector. 例如: RC不支持selector中同一key拥有多个value值(例如: <code>env=devel</code>, <code>env=production</code>), 也不支持selector只包含key, 不包含value.</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1beta2</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ReplicaSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kubia</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">kubia</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">kubia</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">kubia</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">luksa/kubia</span> </span><br></pre></td></tr></table></figure>
<p>ReplicaSet与RC的区别如下:</p>
<ul>
<li>ReplicaSet不属于v1, 而是v1beta2</li>
<li>ReplicaSet的selector为<code>selector.matchLabels</code></li>
</ul>
<p>ReplicaSet支持更复杂的label selector, <code>matchLabels</code>与RC的selector并无区别, <code>matchExpressions</code>则支持更复杂的label匹配:</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">selector:</span></span><br><span class="line">  <span class="attr">matchExpressions:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">app</span></span><br><span class="line">    <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">    <span class="attr">values:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">kubia</span></span><br></pre></td></tr></table></figure>
<p><code>matchExpressions</code>必须包含key和operator, value是可选项. 以下是operator的所有选项:</p>
<ul>
<li>In: pod的value需匹配selector的其中一个value</li>
<li>NotIn: pod的value需与selector中的所有value不同</li>
<li>Exists: pod需拥有selector的key, 不可设置value</li>
<li>DoesNotExist: pod不能拥有selector的key, 不可设置value</li>
</ul>
<p>若同时使用了<code>matchLabels</code>和<code>matchExpressions</code>, 只有全部符合条件的pod才会被ReplicaSet管理.</p>
<h2 id="5-DaemonSet"><a href="#5-DaemonSet" class="headerlink" title="5. DaemonSet"></a>5. DaemonSet</h2><p>RC和ReplicaSet都可让k8s cluster中运行特定数量的pod, 但无法保证哪个pod被分配到了哪个worker node, 有时我们需要每个worker node都运行一个pod, 例如用于收集node信息的log collector或kube-proxy.<br><img src="/images/Kubernetes/replication-5-1.png" alt="DaemonSets run only a single pod replica on each node, whereas ReplicaSets scatter them around the whole cluster randomly"></p>
<p>DaemonSet不需要replica count, 因为pod数量只与worker node的数量相关:</p>
<ul>
<li>当某个node不可用时, DaemonSet不会创建新的pod</li>
<li>当添加一个新的node时, DaemonSet会立即创建新的pod</li>
</ul>
<p>若只想在某些node上分别运行一个pod, 可使用node selector来告诉DaemonSet需要管理哪些node.</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1beta2</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ssd-monitor</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">ssd-monitor</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">ssd-monitor</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">nodeSelector:</span></span><br><span class="line">        <span class="attr">disk:</span> <span class="string">ssd</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">main</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">luksa/ssd-monitor</span></span><br></pre></td></tr></table></figure>
<p>执行<code>kubectl create</code>创建DaemonSet:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl create -f ssd-monitor-daemonset.yaml</span><br><span class="line">daemonset <span class="string">&quot;ssd-monitor&quot;</span> created</span><br><span class="line"></span><br><span class="line">$ kubectl get ds</span><br><span class="line">NAME        DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE-SELECTOR</span><br><span class="line">ssd-monitor 0       0       0     0          0         disk=ssd</span><br></pre></td></tr></table></figure>
<p>以下是DaemonSet的运行结果:<br><img src="/images/Kubernetes/replication-5-2.png" alt="Using a DaemonSet with a node selector to deploy system pods only on certain nodes"></p>
<p>当node移除或修改<code>app: ssd</code> label时, DaemonSet会删除该node上的Pod:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl label node minikube disk=hdd --overwrite</span><br><span class="line">node <span class="string">&quot;minikube&quot;</span> labeled</span><br><span class="line"></span><br><span class="line">$ kubectl get pods</span><br><span class="line">NAME              READY STATUS      RESTARTS AGE</span><br><span class="line">ssd-monitor-hgxwq 1/1   Terminating 0        4m</span><br></pre></td></tr></table></figure>


<h2 id="6-Job"><a href="#6-Job" class="headerlink" title="6. Job"></a>6. Job</h2><p>RC, ReplicaSet和DaemonSet都有一个共同点: 它们会保证管理的pod一直保持运行状态, 当pod中的进程退出时, 它们会自动重启container. 但有时我们只想完成一些一次性工作, 并不需要pod一直运行. Job作为k8s的一种资源, 会在Pod成功运行后自行结束, 以下是pod无法正常执行完毕时的处理:</p>
<ul>
<li>Pod所在的node不可用: job会将该pod分配给其他node<br><img src="/images/Kubernetes/replication-6-1.png" alt="Pods managed by Jobs are rescheduled until they finish successfullys"></li>
<li>Pod运行失败: job可选择是否重启该pod</li>
</ul>
<h3 id="6-1-Define-a-Job-resource"><a href="#6-1-Define-a-Job-resource" class="headerlink" title="6.1 Define a Job resource"></a>6.1 Define a Job resource</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Job</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">batch-job</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">batch-job</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">restartPolicy:</span> <span class="string">OnFailure</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">main</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">luksa/batch-job</span></span><br></pre></td></tr></table></figure>
<p>配置job时需注意以下几点:</p>
<ul>
<li>Job的<code>restartPolicy</code>不可为<code>Always</code>, 因为job需要在pod执行完成后停止</li>
<li>Job中的pod的<code>restartPolicy</code>必须设置为<code>OnFailure</code>或<code>Never</code>, 保证kubelet不会重启已经完成的container</li>
</ul>
<h3 id="6-2-Seeing-a-Job-run-a-Pod"><a href="#6-2-Seeing-a-Job-run-a-Pod" class="headerlink" title="6.2 Seeing a Job run a Pod"></a>6.2 Seeing a Job run a Pod</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">$ kubectl get <span class="built_in">jobs</span></span><br><span class="line">NAME      DESIRED SUCCESSFUL AGE</span><br><span class="line">batch-job 1       0          2s</span><br><span class="line"></span><br><span class="line">$ kubectl get pods</span><br><span class="line">NAME            READY STATUS  RESTARTS AGE</span><br><span class="line">batch-job-28qf4 1/1   Running 0        4s</span><br><span class="line"></span><br><span class="line">----- After pod finish successfully -----</span><br><span class="line"></span><br><span class="line">$ kubectl get pods -a</span><br><span class="line">NAME            READY STATUS    RESTARTS AGE</span><br><span class="line">batch-job-28qf4 0/1   Completed 0        2m</span><br><span class="line"></span><br><span class="line">$ kubectl get job</span><br><span class="line">NAME      DESIRED SUCCESSFUL AGE</span><br><span class="line">batch-job 1       1          3m</span><br></pre></td></tr></table></figure>
<p>需要注意的是, 已经完成的pod默认不会显示在<code>kubectl get pods</code>输出中, 必须加上<code>--show-all</code>或<code>-a</code>才能显示已完成的Pod. </p>
<h3 id="6-3-Run-multiple-pod-instance-in-a-Job"><a href="#6-3-Run-multiple-pod-instance-in-a-Job" class="headerlink" title="6.3 Run multiple pod instance in a Job"></a>6.3 Run multiple pod instance in a Job</h3><p>Job还包含两个重要属性:</p>
<ul>
<li>completions: 总共需要多少个pod完成执行</li>
<li>parallelism: 可同时运行多少个pod</li>
</ul>
<p>通过设置这两个属性, 可实现串行或并行运行pod:</p>
<ol>
<li>Run Job Pods Sequentially<br>  未设置<code>parallelism</code>时, Job会一个个创建pod并等待其运行完毕  <figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">apiVersion: batch/v1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: multi-completion-batch-job</span><br><span class="line">spec:</span><br><span class="line">  completions: 5</span><br><span class="line">  template:</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure></li>
<li>Run Job Pods in Parallel<br>  设置<code>parallelism</code>时, Job会同时运行多个Pod  <figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">apiVersion: batch/v1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: multi-completion-batch-job</span><br><span class="line">spec:</span><br><span class="line">  completions: 5</span><br><span class="line">  parallelism: 2</span><br><span class="line">  template:</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure></li>
</ol>
<p>Job运行时仍可通过<code>kubectl scale</code>修改Job的<code>parallelism</code>. 若pod运行时过长, 可在pod中设置<code>activeDeadlineSeconds</code>, 若pod运行时间超过该值, k8s会判断job失败并终止当前pod.</p>
<h2 id="7-CronJob"><a href="#7-CronJob" class="headerlink" title="7. CronJob"></a>7. CronJob</h2><p>Job会在其创建时立即运行pod, 无法在某个特定时刻运行pod. CronJob作为k8s的一种资源, 具有定时执行功能, 精确到秒. 假设我们需要每隔15分钟执行一次任务:</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">apiVersion: batch/v1beta1</span><br><span class="line">kind: CronJob</span><br><span class="line">metadata:</span><br><span class="line">  name: batch-job-every-fifteen-minutes</span><br><span class="line">spec:</span><br><span class="line">  schedule: <span class="string">&quot;0,15,30,45 * * * *&quot;</span></span><br><span class="line">  jobTemplate:</span><br><span class="line">    spec:</span><br><span class="line">      template:</span><br><span class="line">        metadata:</span><br><span class="line">          labels:</span><br><span class="line">            app: periodic-batch-job</span><br><span class="line">        spec:</span><br><span class="line">          restartPolicy: OnFailure</span><br><span class="line">          containers:</span><br><span class="line">          - name: main</span><br><span class="line">            image: luksa/batch-job </span><br></pre></td></tr></table></figure>
<p>CronJob的<code>schedule</code>属性分为以下5个值:</p>
<ul>
<li>Minute</li>
<li>Hour</li>
<li>Day of month</li>
<li>Month</li>
<li>Day of week</li>
</ul>
<p>若需每个月第一天每隔30分钟执行一次, 可设置为<code>0, 30 * 1 * *</code>; 若需每个周日的3AM执行, 可设置为<code>0 3 * * 0</code>. 一般来说, pod的创建时间会比指定时间晚一点, 若对pod的执行时刻有严格限制, 可在YAML文件中设置<code>startingDeadlineSeconds</code>. 例如: </p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">CronJob</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">schedule:</span> <span class="string">&quot;30 10 * * *&quot;</span></span><br><span class="line">  <span class="attr">startingDeadlineSeconds:</span> <span class="number">15</span></span><br><span class="line">  <span class="string">...</span></span><br></pre></td></tr></table></figure>
<p>上述例子中, 若pod未能在10:30:15之前执行, 则不会运行pod, 并将其标记为<code>Failed</code>.</p>

    </div>

    <footer class="post-footer">
      
      <div class="post-tags">
        
          <a href="/tags/K8s/">K8s</a>
        
      </div>
      

      
      
  <nav class="post-nav">
    
      <a class="prev" href="/p/1936.html">
        <i class="icon-left"></i>
        <span class="prev-text nav-default">Services</span>
        <span class="prev-text nav-mobile">Prev</span>
      </a>
    
    
      <a class="next" href="/p/f841.html">
        <span class="next-text nav-default">Pods</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="icon-right"></i>
      </a>
    
  </nav>

      
    </footer>
  </article>

  </div>
  
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    showProcessingMessages: true,
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: false,
        skipTags: ["script","noscript","style","textarea"]
    },
    TeX: {
        Macros:{
            Arr: ["\\{ #1 \\}", 1],
            fi: "{f\\,\\!_i}",
            SS: ["{#1\\:\\!_#2}",2],
            SUBx: ["{\\:\\!_#1}",1],
            EXPx: ["{\\;\\!^#1}",1],
            Ss: ["{#1\\,\\!_#2}",2],
            subx: ["{\\,\\!_#1}",1]
        }
    }
});
</script>


      </div>
    </div>

    
<script type="text/javascript">
  var disqus_shortname = 'zaf1ro';
  var disqus_identifier = 'p/17e0.html';
  var disqus_title = "Replication Controllers";

  var disqus = {
    load : function disqus(){
        if(typeof DISQUS !== 'object') {
          (function () {
          var s = document.createElement('script'); s.async = true;
          s.type = 'text/javascript';
          s.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
          }());
          $('#load-disqus').remove(); ///加载后移除按钮
        }
    }
  }

  
    var disqus_config = function () {
        this.page.url = disqus_url;
        this.page.identifier = disqus_identifier;
        this.page.title = disqus_title;
    };
  

</script>



    




  
    <script type="text/javascript" src="/lib/jquery/jquery-3.6.0.min.js"></script>
  

  

    
    <script type="text/javascript">
(function(){"use strict";var Theme={};Theme.backToTop={register:function(){var $backToTop=$('#back-to-top');$(window).scroll(function(){if($(window).scrollTop()>100){$backToTop.fadeIn(1000)}else{$backToTop.fadeOut(1000)}});$backToTop.click(function(){$('body').animate({scrollTop:0})})}};Theme.fancybox={register:function(){if($.fancybox){$('.post').each(function(){$(this).find('img').each(function(){$(this).wrap('<a class="fancybox" href="'+this.src+'" title="'+this.alt+'"></a>')})});$('.fancybox').fancybox({openEffect:'elastic',closeEffect:'elastic'})}}};this.Theme=Theme}.call(this));
</script>

<script type="text/javascript">
$(document).ready(function(){if(themeConfig.fancybox.enable){Theme.fancybox.register()}Theme.backToTop.register()});
</script>
    
<script type="text/javascript">
var themeConfig = {
  fancybox: {
    enable: false
  },
};
</script>

    <script>
    $('table').wrap('<div style="overflow-x: auto;"></div>');
</script>
    
    <script>
$(document).ready(function () {
    $('#sidebarCollapse').on('click', function(){
        $('#sidebar').toggleClass('active');
    });

    $('#toc ol li a').on('click', function(){
        $('#sidebar').toggleClass('active');
    });
});
</script>
  </body>
</html>
