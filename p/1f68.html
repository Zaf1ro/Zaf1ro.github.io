<!DOCTYPE html>
<html lang="zh">
  <head>
    
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1">



  <meta name="description" content="System Design - Web Crawler"/>




  <meta name="keywords" content="System Design," />


<meta name="google-site-verification" content="qy4gf0StWj627u-7aIP3WDCLBi3jPYXhm57UC7TUcok" />
<meta name="baidu-site-verification" content="XJoPG7ad2Z" />

<link rel="alternate" hreflang="x-default" href="https://zaf1ro.github.io/p/1f68.html" />
<link rel="alternate" hreflang="zh" href="https://zaf1ro.github.io/p/1f68.html" />




  <link rel="alternate" href="/default" title="Zaf1ro" >




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.jpeg?v=1.1" />



<link rel="canonical" href="https://zaf1ro.github.io/p/1f68.html"/>


<meta name="description" content="1. IntroductionWeb crawler is used by search engines to discover new or updated content on the web. A web crawler starts by collecting a few web pages and then follows links on those pages to collect">
<meta property="og:type" content="article">
<meta property="og:title" content="System Design - Web Crawler">
<meta property="og:url" content="https://zaf1ro.github.io/p/1f68.html">
<meta property="og:site_name" content="Zaf1ro">
<meta property="og:description" content="1. IntroductionWeb crawler is used by search engines to discover new or updated content on the web. A web crawler starts by collecting a few web pages and then follows links on those pages to collect">
<meta property="og:locale">
<meta property="og:image" content="https://zaf1ro.github.io/images/System-Design/Interview/9-high-level-design.jpg">
<meta property="og:image" content="https://zaf1ro.github.io/images/System-Design/Interview/9-web-crawler-workflow.jpg">
<meta property="og:image" content="https://zaf1ro.github.io/images/System-Design/Interview/9-queue-router-politeness.jpg">
<meta property="og:image" content="https://zaf1ro.github.io/images/System-Design/Interview/9-add-new-modules.jpg">
<meta property="article:published_time" content="2024-04-15T16:42:10.000Z">
<meta property="article:modified_time" content="2024-08-20T16:43:04.055Z">
<meta property="article:tag" content="System Design">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zaf1ro.github.io/images/System-Design/Interview/9-high-level-design.jpg">


<link rel="stylesheet" type="text/css" href="/css/style.css?v=1.1" />




    <title>
System Design - Web Crawler - Zaf1ro
</title>
  <meta name="generator" content="Hexo 6.3.0"></head>

  <body>
  <nav id="sidebar" class="active on-post">
    <div id="third">
      <div id="sidebar-title">
        <h1 id="sidebar-title-text">
            <a href="/." class="logo">Home</a>
        </h1>
      </div>
      <div id="google-search">
  <script async src="https://cse.google.com/cse.js?cx=009060789867951546370:v3hkcobeuh9"></script>
  <div class="gcse-search"></div>
</div>
      
  <div id="toc">
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Introduction"><span class="toc-text">1. Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Understand-the-Problem-and-Establish-Design-Scope"><span class="toc-text">2. Understand the Problem and Establish Design Scope</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Questions"><span class="toc-text">2.1 Questions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Characteristics-of-a-Good-Web-Crawler"><span class="toc-text">2.2 Characteristics of a Good Web Crawler</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Back-of-the-Envelope-Estimation"><span class="toc-text">2.3 Back of the Envelope Estimation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Propose-high-level-design-and-get-buy-in"><span class="toc-text">3. Propose high-level design and get buy-in</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Seed-URLs"><span class="toc-text">3.1 Seed URLs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-URL-Frontier"><span class="toc-text">3.2 URL Frontier</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-HTML-Downloader"><span class="toc-text">3.3 HTML Downloader</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-DNS-Resolver"><span class="toc-text">3.4 DNS Resolver</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-Content-Parser"><span class="toc-text">3.5 Content Parser</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-Content-Seen"><span class="toc-text">3.6 Content Seen?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-7-Content-Storage"><span class="toc-text">3.7 Content Storage</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-8-URL-Extractor"><span class="toc-text">3.8 URL Extractor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-9-URL-Filter"><span class="toc-text">3.9 URL Filter</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-10-URL-Seen"><span class="toc-text">3.10 URL Seen?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-11-URL-Storage"><span class="toc-text">3.11 URL Storage</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-12-Web-Crawler-Workflow"><span class="toc-text">3.12 Web Crawler Workflow</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Design-deep-dive"><span class="toc-text">4. Design deep dive</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-DFS-vs-BFS"><span class="toc-text">4.1 DFS vs BFS</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-URL-frontier"><span class="toc-text">4.1 URL frontier</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-1-Politeness"><span class="toc-text">4.1.1 Politeness</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-2-Priority"><span class="toc-text">4.1.2 Priority</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-3-Freshness"><span class="toc-text">4.1.3 Freshness</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-4-Storage-for-URL-Frontier"><span class="toc-text">4.1.4 Storage for URL Frontier</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-HTML-Downloader"><span class="toc-text">4.2 HTML Downloader</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-1-Robots-txt"><span class="toc-text">4.2.1 Robots.txt</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-2-Performance-optimization"><span class="toc-text">4.2.2 Performance optimization</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-Robustness"><span class="toc-text">4.3 Robustness</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-Extensibility"><span class="toc-text">4.4 Extensibility</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-Detect-and-avoid-problematic-content"><span class="toc-text">4.5 Detect and avoid problematic content</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Wrap-up"><span class="toc-text">5. Wrap up</span></a></li></ol>
  </div>

    </div>
  </nav>

    <div id="page">
      <header id="masthead"><div class="site-header-inner">
  
  <button class="nav-mobile-button on-post" id="sidebarCollapse">
  
    <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24"><path d="M24 6h-24v-4h24v4zm0 4h-24v4h24v-4zm0 8h-24v4h24v-4z"/></svg>
  </button>
  
  


  <nav id="nav-top">
    
      
      <ul id="menu-top" class="nav-top-items on-post">
      
        
          <li class="menu-item">
            <a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/Zaf1ro">
              
              
                Github
              
            </a>
          </li>
        
      </ul>
    
  </nav>
</div>

      </header>
      <div id="content">
        
  <div class="primary">
    
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          System Design - Web Crawler
        
      </h1>
      <time class="post-time">
          04/15/24
      </time>
    </header>

    <div class="post-content">
      <h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>Web crawler is used by search engines to discover new or updated content on the web. A web crawler starts by collecting a few web pages and then follows links on those pages to collect new content.<br>A crawler is used for many purposes:</p>
<ul>
<li>Search engine indexing: collect web pages to create a local index for search engines</li>
<li>Web archiving: collect information from the web to preserve<br>data for future uses</li>
<li>Web mining: help to discover useful knowledge from the internet</li>
<li>Web monitoring: help to monitor copyright and trademark infringements over the Internet</li>
</ul>
<h2 id="2-Understand-the-Problem-and-Establish-Design-Scope"><a href="#2-Understand-the-Problem-and-Establish-Design-Scope" class="headerlink" title="2. Understand the Problem and Establish Design Scope"></a>2. Understand the Problem and Establish Design Scope</h2><p>Basic algorithm of a web crawler:</p>
<ol>
<li>Given a set of URLs, download all the web pages addressed by the URLs</li>
<li>Extract URLs from these web pages</li>
<li>Add new URLs to the list of URLs to be downloaded</li>
</ol>
<h3 id="2-1-Questions"><a href="#2-1-Questions" class="headerlink" title="2.1 Questions"></a>2.1 Questions</h3><ul>
<li>What is the main purpose of the crawler: Search engine indexing</li>
<li>How many web pages does the web crawler collect per month: 1 billion pages</li>
<li>What content types are included: HTML only</li>
<li>Shall we consider newly added or edited web pages: Yes, both</li>
<li>Do we need to store HTML pages crawled from the web: Yes, up to 5 years</li>
<li>How do we handle web pages with duplicate content: Pages with duplicate content should be ignored</li>
</ul>
<h3 id="2-2-Characteristics-of-a-Good-Web-Crawler"><a href="#2-2-Characteristics-of-a-Good-Web-Crawler" class="headerlink" title="2.2 Characteristics of a Good Web Crawler"></a>2.2 Characteristics of a Good Web Crawler</h3><ul>
<li>Scalability: Web crawling should be efficient by using parallelization</li>
<li>Robustness: web crawler should handle bad HTML, unresponsive servers, crashes, malicious links</li>
<li>Politeness: web crawler should not make too many requests to a website in a short time interval</li>
<li>Extensibility: minimal changes are needed to support new content types</li>
</ul>
<h3 id="2-3-Back-of-the-Envelope-Estimation"><a href="#2-3-Back-of-the-Envelope-Estimation" class="headerlink" title="2.3 Back of the Envelope Estimation"></a>2.3 Back of the Envelope Estimation</h3><ul>
<li>Assume 1 billion web pages are downloaded every month</li>
<li>QPS: 1,000,000,000 &#x2F; 30 days &#x2F; 24 hours &#x2F; 3600 seconds &#x3D; 400 pages per second</li>
<li>Peak QPS &#x3D; 2 * QPS &#x3D; 800 pages per second</li>
<li>Assume the average web page size is 500k</li>
<li>1,000,000,000 x 500k &#x3D; 500 TB storage per month</li>
<li>500 TB * 12 months * 5 years &#x3D; 30 PB for five years</li>
</ul>
<h2 id="3-Propose-high-level-design-and-get-buy-in"><a href="#3-Propose-high-level-design-and-get-buy-in" class="headerlink" title="3. Propose high-level design and get buy-in"></a>3. Propose high-level design and get buy-in</h2><p><img src="/images/System-Design/Interview/9-high-level-design.jpg" alt="High Level Design"></p>
<h3 id="3-1-Seed-URLs"><a href="#3-1-Seed-URLs" class="headerlink" title="3.1 Seed URLs"></a>3.1 Seed URLs</h3><p>A web crawler uses seed URLs as a starting point for the crawl process. A good seed URL serves as a good starting point that a crawler can utilize to traverse as many links as possible. The general strategy is to divide the entire URL space into smaller ones:</p>
<ul>
<li>Based on locality as different countries may have different popular websites</li>
<li>Based on topics</li>
</ul>
<h3 id="3-2-URL-Frontier"><a href="#3-2-URL-Frontier" class="headerlink" title="3.2 URL Frontier"></a>3.2 URL Frontier</h3><p>Most modern web crawlers split the crawl state into two:</p>
<ul>
<li>to be downloaded</li>
<li>already downloaded</li>
</ul>
<p>The component that stores URLs to be downloaded is called the <strong>URL Frontier</strong>, implemented by a First-in-First-out (FIFO) queue.</p>
<h3 id="3-3-HTML-Downloader"><a href="#3-3-HTML-Downloader" class="headerlink" title="3.3 HTML Downloader"></a>3.3 HTML Downloader</h3><p>The HTML downloader downloads web pages from the internet. Those URLs are provided by the URL Frontier.</p>
<h3 id="3-4-DNS-Resolver"><a href="#3-4-DNS-Resolver" class="headerlink" title="3.4 DNS Resolver"></a>3.4 DNS Resolver</h3><p>To download a web page, a URL must be translated into an IP address. The HTML Downloader calls the DNS Resolver to get the corresponding IP address for the URL.</p>
<h3 id="3-5-Content-Parser"><a href="#3-5-Content-Parser" class="headerlink" title="3.5 Content Parser"></a>3.5 Content Parser</h3><p>After a web page is downloaded, it must be parsed and validated because malformed web pages could provoke problems and waste storage space.</p>
<h3 id="3-6-Content-Seen"><a href="#3-6-Content-Seen" class="headerlink" title="3.6 Content Seen?"></a>3.6 Content Seen?</h3><p>Online research reveals that 29% of the web pages are duplicated contents, which may cause the same content to be stored multiple times. This component is to eliminate data redundancy and shorten processing time. To compare two HTML documents:</p>
<ul>
<li>compare character by character: slow and time-consuming</li>
<li>compare the hash values of the two web pages</li>
</ul>
<h3 id="3-7-Content-Storage"><a href="#3-7-Content-Storage" class="headerlink" title="3.7 Content Storage"></a>3.7 Content Storage</h3><p>Storage system for storing HTML content. The choice of storage system depends on factors such as data type, data size, access frequency, life span, etc.</p>
<ul>
<li>Most of the content is stored on disk because data set is too big to fit in memory</li>
<li>Popular content is kept in memory to reduce latency</li>
</ul>
<h3 id="3-8-URL-Extractor"><a href="#3-8-URL-Extractor" class="headerlink" title="3.8 URL Extractor"></a>3.8 URL Extractor</h3><p>Parse and extract links from HTML pages. Relative paths are converted to absolute URLs by adding website prefix.</p>
<h3 id="3-9-URL-Filter"><a href="#3-9-URL-Filter" class="headerlink" title="3.9 URL Filter"></a>3.9 URL Filter</h3><p>Excludes certain content types, file extensions, error links and URLs in &quot;blacklisted&quot; sites.</p>
<h3 id="3-10-URL-Seen"><a href="#3-10-URL-Seen" class="headerlink" title="3.10 URL Seen?"></a>3.10 URL Seen?</h3><p>A data structure that keeps track of URLs that are visited before or already in the Frontier. Bloom filter and hash table are common techniques to implement the component. </p>
<h3 id="3-11-URL-Storage"><a href="#3-11-URL-Storage" class="headerlink" title="3.11 URL Storage"></a>3.11 URL Storage</h3><p>Stores already visited URLs.</p>
<h3 id="3-12-Web-Crawler-Workflow"><a href="#3-12-Web-Crawler-Workflow" class="headerlink" title="3.12 Web Crawler Workflow"></a>3.12 Web Crawler Workflow</h3><p><img src="/images/System-Design/Interview/9-web-crawler-workflow.jpg" alt="Web Crawler Workflow"></p>
<ol>
<li>Add seed URLs to the <code>URL Frontier</code></li>
<li><code>HTML Downloader</code> fetches a list of URLs from <code>URL Frontier</code></li>
<li><code>HTML Downloader</code> gets IP addresses of URLs from <code>DNS resolver</code> and starts downloading</li>
<li><code>Content Parser</code> parses HTML pages and checks if pages are malformed, and passed to the <code>Content Seen?</code></li>
<li><code>Content Seen?</code> checks if a HTML page is already in the storage<ul>
<li>If it is in the storage, the HTML page is discarded</li>
<li>If it is not in the storage, passed to <code>Link Extractor</code></li>
</ul>
</li>
<li><code>Link extractor</code> extracts links from HTML pages, and passed to the <code>URL Seen?</code></li>
<li><code>URL Seen?</code> checks if a URL is already in the storage. if yes, it is processed before, and nothing needs to be done.</li>
<li>If a URL has not been processed before, added to the <code>URL Frontier</code></li>
</ol>
<h2 id="4-Design-deep-dive"><a href="#4-Design-deep-dive" class="headerlink" title="4. Design deep dive"></a>4. Design deep dive</h2><h3 id="4-1-DFS-vs-BFS"><a href="#4-1-DFS-vs-BFS" class="headerlink" title="4.1 DFS vs BFS"></a>4.1 DFS vs BFS</h3><p>web as a directed graph, web pages serve as nodes and hyperlinks. Web crawl process can be seen as traversing a directed graph from one page to others. DFS is usually not a good choice because the depth of DFS can be very deep.<br>BFS is commonly used by web crawlers and is implemented by a first-in-first-out(FIFO) queue. This implementation has two problems:</p>
<ul>
<li>Most links from the same web page are linked back to the same host. When the crawler tries to download web pages in parallel, servers will be flooded with requests. This is considered as &quot;impolite&quot;.</li>
<li>Standard BFS does not take the priority of a URL into consideration, not every page has the same level of quality and importance.</li>
</ul>
<h3 id="4-1-URL-frontier"><a href="#4-1-URL-frontier" class="headerlink" title="4.1 URL frontier"></a>4.1 URL frontier</h3><p>A URL frontier is a data structure that stores URLs to be downloaded. The URL frontier is an important component to ensure politeness, URL prioritization, and freshness.</p>
<h4 id="4-1-1-Politeness"><a href="#4-1-1-Politeness" class="headerlink" title="4.1.1 Politeness"></a>4.1.1 Politeness</h4><p>A web crawler should avoid sending too many requests to the same hosting server within a short period.<br><img src="/images/System-Design/Interview/9-queue-router-politeness.jpg" alt="Web Crawler Politeness"></p>
<ul>
<li>Queue router: It ensures that each queue (b1, b2, … bn) only contains URLs from the same host</li>
<li>Mapping table: It maps each host to a queue</li>
<li>FIFO queues b1, b2 to bn: Each queue contains URLs from the same host</li>
<li>Queue selector: Each worker is mapped to a FIFO queue, and it only downloads URLs from that queue</li>
<li>A worker only downloads web pages from the same host. A delay can be added between two download tasks.</li>
</ul>
<h4 id="4-1-2-Priority"><a href="#4-1-2-Priority" class="headerlink" title="4.1.2 Priority"></a>4.1.2 Priority</h4><p>We prioritize URLs based on usefulness, which can be measured by PageRank, website traffic, update frequency, etc.</p>
<ol>
<li>The input URL will be passed to prioritizer(which handles URL prioritization)</li>
<li>Queue f1 to fn: Each queue has an assigned priority.</li>
<li>Queue selector: Randomly choose a queue with a bias towards queues with higher priority.</li>
</ol>
<h4 id="4-1-3-Freshness"><a href="#4-1-3-Freshness" class="headerlink" title="4.1.3 Freshness"></a>4.1.3 Freshness</h4><p>A web crawler must periodically recrawl downloaded pages to keep our data set fresh. Few strategies to optimize freshness:</p>
<ul>
<li>Recrawl based on web pages&#39; update history</li>
<li>Prioritize URLs and recrawl important pages first and more frequently</li>
</ul>
<h4 id="4-1-4-Storage-for-URL-Frontier"><a href="#4-1-4-Storage-for-URL-Frontier" class="headerlink" title="4.1.4 Storage for URL Frontier"></a>4.1.4 Storage for URL Frontier</h4><p>the number of URLs in the frontier could be hundreds of millions: The majority of URLs are stored on disk, maintain buffers in memory for enqueue&#x2F;dequeue operations.</p>
<h3 id="4-2-HTML-Downloader"><a href="#4-2-HTML-Downloader" class="headerlink" title="4.2 HTML Downloader"></a>4.2 HTML Downloader</h3><h4 id="4-2-1-Robots-txt"><a href="#4-2-1-Robots-txt" class="headerlink" title="4.2.1 Robots.txt"></a>4.2.1 Robots.txt</h4><p>Robots.txt, called Robots Exclusion Protocol, is a standard used by websites to communicate with crawlers. It specifies what pages crawlers are allowed to download.</p>
<h4 id="4-2-2-Performance-optimization"><a href="#4-2-2-Performance-optimization" class="headerlink" title="4.2.2 Performance optimization"></a>4.2.2 Performance optimization</h4><ol>
<li>Distributed crawl: To achieve high performance, crawl jobs are distributed into multiple servers, each downloader is responsible for a subset of the URLs.</li>
<li>Cache DNS Resolver: DNS cache keeps the <strong>domain name</strong> to <strong>IP address</strong> mapping and is updated periodically by cron jobs.</li>
<li>Locality: Distribute crawl servers geographically. When crawl servers are closer to website hosts, crawlers experience faster download time.</li>
<li>Short timeout: If a host does not respond within a predefined time, the crawler will stop the job and crawl some other pages.</li>
</ol>
<h3 id="4-3-Robustness"><a href="#4-3-Robustness" class="headerlink" title="4.3 Robustness"></a>4.3 Robustness</h3><ul>
<li>Consistent hashing: distribute loads among downloaders, a new downloader server can be added or removed using consistent hashing</li>
<li>Save crawl states and data: To guard against failures, crawl states and data are written to a storage system.</li>
<li>Exception handling: web crawler must handle exceptions</li>
<li>Data validation</li>
</ul>
<h3 id="4-4-Extensibility"><a href="#4-4-Extensibility" class="headerlink" title="4.4 Extensibility"></a>4.4 Extensibility</h3><p>The crawler can be extended by plugging in new modules.<br><img src="/images/System-Design/Interview/9-add-new-modules.jpg" alt="Web Crawler Add Modules"></p>
<ul>
<li>PNG Downloader module is plugged-in to download PNG files.</li>
<li>Web Monitor module is added to monitor the web and prevent copyright and trademark infringements</li>
</ul>
<h3 id="4-5-Detect-and-avoid-problematic-content"><a href="#4-5-Detect-and-avoid-problematic-content" class="headerlink" title="4.5 Detect and avoid problematic content"></a>4.5 Detect and avoid problematic content</h3><ul>
<li>Redundant content: Hashes or checksums</li>
<li>Spider traps: A spider trap is a web page that causes a crawler in an infinite loop. For instance, an infinite deep directory structure is like: <code>www.example.com/foo/bar/foo/bar/foo/bar/...</code>. Such spider traps can be avoided by setting a maximal length for URLs, but it&#39;s hard to develop automatic algorithms to avoid all types of spider traps. </li>
<li>Data noise: advertisements, code snippets, spam URLs should be excluded</li>
</ul>
<h2 id="5-Wrap-up"><a href="#5-Wrap-up" class="headerlink" title="5. Wrap up"></a>5. Wrap up</h2><p>Additional talking points:</p>
<ul>
<li>Server-side rendering: some website use scripts like JavaScript and AJAX to generate links. If we download and parse web pages directly, we will not be able to retrieve dynamically generated links.</li>
<li>Filter out unwanted pages: an anti-spam component to filter out low quality and spam pages</li>
<li>Database replication and sharding: replication and sharding are used to improve the data layer availability, scalability, and reliability</li>
<li>Horizontal scaling: keep servers stateless -&gt; thousands of servers perform download tasks</li>
<li>Analytics: collect and analyze data</li>
</ul>

    </div>

    <footer class="post-footer">
      
      <div class="post-tags">
        
          <a href="/tags/System-Design/">System Design</a>
        
      </div>
      

      
      
  <nav class="post-nav">
    
      <a class="prev" href="/p/9f54.html">
        <i class="icon-left"></i>
        <span class="prev-text nav-default">System Design - Notification System</span>
        <span class="prev-text nav-mobile">Prev</span>
      </a>
    
    
      <a class="next" href="/p/6100.html">
        <span class="next-text nav-default">DP - Complete Knapsack Problem</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="icon-right"></i>
      </a>
    
  </nav>

      
    </footer>
  </article>

  </div>
  
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    showProcessingMessages: true,
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: false,
        skipTags: ["script","noscript","style","textarea"]
    },
    TeX: {
        Macros:{
            Arr: ["\\{ #1 \\}", 1],
            fi: "{f\\,\\!_i}",
            SS: ["{#1\\:\\!_#2}",2],
            SUBx: ["{\\:\\!_#1}",1],
            EXPx: ["{\\;\\!^#1}",1],
            Ss: ["{#1\\,\\!_#2}",2],
            subx: ["{\\,\\!_#1}",1]
        }
    }
});
</script>


      </div>
    </div>

    
<script type="text/javascript">
  var disqus_shortname = 'zaf1ro';
  var disqus_identifier = 'p/1f68.html';
  var disqus_title = "System Design - Web Crawler";

  var disqus = {
    load : function disqus(){
        if(typeof DISQUS !== 'object') {
          (function () {
          var s = document.createElement('script'); s.async = true;
          s.type = 'text/javascript';
          s.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
          }());
          $('#load-disqus').remove(); ///加载后移除按钮
        }
    }
  }

  
    var disqus_config = function () {
        this.page.url = disqus_url;
        this.page.identifier = disqus_identifier;
        this.page.title = disqus_title;
    };
  

</script>



    




  
    <script type="text/javascript" src="/lib/jquery/jquery-3.6.0.min.js"></script>
  

  

    
    <script type="text/javascript">
(function(){"use strict";var Theme={};Theme.backToTop={register:function(){var $backToTop=$('#back-to-top');$(window).scroll(function(){if($(window).scrollTop()>100){$backToTop.fadeIn(1000)}else{$backToTop.fadeOut(1000)}});$backToTop.click(function(){$('body').animate({scrollTop:0})})}};Theme.fancybox={register:function(){if($.fancybox){$('.post').each(function(){$(this).find('img').each(function(){$(this).wrap('<a class="fancybox" href="'+this.src+'" title="'+this.alt+'"></a>')})});$('.fancybox').fancybox({openEffect:'elastic',closeEffect:'elastic'})}}};this.Theme=Theme}.call(this));
</script>

<script type="text/javascript">
$(document).ready(function(){if(themeConfig.fancybox.enable){Theme.fancybox.register()}Theme.backToTop.register()});
</script>
    
<script type="text/javascript">
var themeConfig = {
  fancybox: {
    enable: false
  },
};
</script>

    <script>
    $('table').wrap('<div style="overflow-x: auto;"></div>');
</script>
    
    <script>
$(document).ready(function () {
    $('#sidebarCollapse').on('click', function(){
        $('#sidebar').toggleClass('active');
    });

    $('#toc ol li a').on('click', function(){
        $('#sidebar').toggleClass('active');
    });
});
</script>
  </body>
</html>
